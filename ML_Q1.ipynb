{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Final Exam\n",
    "\n",
    "### Question 1:  Classification Task (15 Marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given “Health Monitoring”dataset has 3000 rows ,4 features (Heart_Rate, Blood_Pressure,\n",
    "Cholesterol, Blood_Sugar), a target variable (Risk_Level) and some missing values. The target\n",
    "dataset has 3 classes ('Low', 'Medium', or 'High'). Perform the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split,  GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to the DataFrame\n",
    "df_ML_q1 = pd.read_csv('Health_Monitoring_System_Data.csv')\n",
    "\n",
    "print(df_ML_q1.shape)\n",
    "df_ML_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_q1.describe()\n",
    "\n",
    "#df_ML_q1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "df_ML_q1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Data Preprocessing\n",
    "\n",
    "There are a few methods in handling missing values.  The following are some of that we can consider using.  \n",
    "1) Dropping missing value: \n",
    "Dropping Missing Values:\n",
    "    Benefits:\n",
    "        - Simple and straightforward.\n",
    "        - Preserves the structure of the dataset.\n",
    "    Shortcomings:\n",
    "        - May result in loss of a significant amount of data, especially if there are many missing values.\n",
    "        - If the missing values are not completely at random, dropping them may introduce bias.\n",
    "\n",
    "2) Imputation using Mean/Median/Mode:\n",
    "    Benefits:\n",
    "        - Easy to implement.\n",
    "        - Preserves the number of observations in the dataset.\n",
    "        - Works well for variables with a normal distribution.\n",
    "    Shortcomings:\n",
    "        - Ignores relationships between features.\n",
    "        - Can distort the distribution and relationships in the data.\n",
    "        - May not be suitable for variables with skewed distributions.\n",
    "    \n",
    "3) Imputation using Predictive Models (e.g., Regression):\n",
    "    Benefits:\n",
    "        - Utilizes relationships between variables.\n",
    "        - Preserves the variability and structure of the data.\n",
    "    Shortcomings:\n",
    "        - Assumes the relationship between variables is linear.\n",
    "        - May not work well if the dataset is small or if the relationship is complex.\n",
    "        - Sensitive to outliers.\n",
    "\n",
    "4) Multiple Imputation:\n",
    "    Benefits:\n",
    "        - Takes into account the uncertainty associated with imputed values.\n",
    "        - Preserves the variance and relationships in the data.\n",
    "    Shortcomings:\n",
    "        - More computationally expensive.\n",
    "        - Requires assumptions about the distribution of the data.\n",
    "\n",
    "5) Imputation using K-Nearest Neighbors (KNN):\n",
    "    Benefits:\n",
    "        - Considers relationships between variables.\n",
    "        - Non-parametric and can handle non-linear relationships.\n",
    "    Shortcomings:\n",
    "        - Sensitive to the choice of the number of neighbors (K).\n",
    "        - Computationally expensive for large datasets.\n",
    "\n",
    "The choice of the strategy depends on the nature of the data, the extent of missingness, and the goals of the analysis. It's often recommended to explore the data, understand the missing data mechanisms, and select an approach that aligns with the specific characteristics of the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the nature of missing values; \n",
    "# if the rows with missing value in 'Heart_Rate' have missing values in 'Blood_Sugar' as well.\n",
    "\n",
    "# Check if missing values in 'Heart_Rate' correspond to missing values in 'Blood_Sugar'\n",
    "missing_heart_rate = df_ML_q1['Heart_Rate'].isnull()\n",
    "missing_blood_sugar = df_ML_q1['Blood_Sugar'].isnull()\n",
    "\n",
    "# Check if all rows (index) with missing 'Heart_Rate' also have missing 'Blood_Sugar'\n",
    "all_missing_heart_rate_have_missing_blood_sugar = missing_heart_rate.equals(missing_blood_sugar)\n",
    "\n",
    "# Print the result\n",
    "print(f\"All rows with missing 'Heart_Rate' also missing 'Blood_Sugar': {all_missing_heart_rate_have_missing_blood_sugar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no missing value on the target column, 'Risk_Level', we can use machine learning (ML) imputation methods based on the other columns to populate the missing values in the 'Heart_Rate' and 'Blood_Sugar' columns.\n",
    "\n",
    "In this predictive imputation method, we train a machine learning model on the rows where the target column has values (this is the case in our dataset).  The model will then predict the missing values in the columns.\n",
    "\n",
    "Here's a general outline of the steps:\n",
    "    - Identify the target column; 'Heart_Rate' and 'Blood_Sugar'.\n",
    "    - Split the dataset into two parts: Training set containing all rows without missing values, and Test_set containing rows with missing values.\n",
    "    - Use the non-missing values in other columns as features as variables to train the model.\n",
    "    . Use the trained model to predict the missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of imputation model, including whether to use KNeighborsRegressor, depends on various factors such as the nature of your data, the distribution of missing values, and the underlying relationships between variables. KNeighborsRegressor is just one option, and its suitability can vary based on your specific use case. Here are some reasons why KNeighborsRegressor might be recommended:\n",
    "\n",
    "Local Relationships: KNeighborsRegressor imputes missing values by considering the values of the nearest neighbors. This can be beneficial if the underlying relationships in your data are local or if there is spatial/temporal structure. For example, similar individuals might have similar heart rates.\n",
    "\n",
    "Non-linearity: If the relationship between the features and the target variable (Heart_Rate or Blood_Sugar) is non-linear, KNeighborsRegressor can capture these non-linear patterns. Other imputation techniques like mean imputation or linear regression assume linearity, which may not be suitable for all datasets.\n",
    "\n",
    "Flexibility: KNeighborsRegressor is a non-parametric method and doesn't make strong assumptions about the distribution of the data. It can adapt to complex patterns without assuming a specific functional form.\n",
    "\n",
    "However, it's important to note the limitations as well:\n",
    "\n",
    "Computational Cost: Calculating distances to find nearest neighbors can be computationally expensive, especially with large datasets. Other imputation methods, like mean imputation or regression imputation, might be computationally more efficient.\n",
    "\n",
    "Sensitivity to Noise: If your dataset has noise or outliers, KNeighborsRegressor might be sensitive to them, potentially leading to less robust imputation results.\n",
    "\n",
    "Hyperparameter Tuning: The performance of KNeighborsRegressor depends on the choice of hyperparameters, such as the number of neighbors (n_neighbors). Tuning these parameters is crucial for achieving good imputation results.\n",
    "\n",
    "In practice, it's often a good idea to try multiple imputation methods, including both parametric and non-parametric approaches, and evaluate their performance using cross-validation or other validation strategies to choose the method that works best for your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using KNeighbors\n",
    "\n",
    "\n",
    "# Use KNeighborsRegressor model to impute missing values in 'Heart_Rate' and 'Blood_Sugar' columns\n",
    "columns_to_impute = ['Heart_Rate', 'Blood_Sugar']\n",
    "\n",
    "# Create a copy of the DataFrame for imputation\n",
    "df_imputed = df_ML_q1.copy()\n",
    "\n",
    "# Define a range of n_neighbors values to try\n",
    "neighbor_values = list(range(2, 11))\n",
    "\n",
    "# Initialize variables to keep track of the best imputation\n",
    "best_imputation = None\n",
    "best_n_neighbors = None\n",
    "smallest_nulls = float('inf')  # Initialize with a large value\n",
    "\n",
    "for n_neighbors in neighbor_values:\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Perform imputation for both columns\n",
    "    df_temp = df_imputed.copy()  # Create a temporary DataFrame for each iteration\n",
    "    df_temp[columns_to_impute] = imputer.fit_transform(df_temp.drop(columns_to_impute + ['Risk_Level'], axis=1))\n",
    "\n",
    "    # Check the number of remaining null values\n",
    "    nulls = df_temp[columns_to_impute].isnull().sum().sum()\n",
    "\n",
    "    # Update the best imputation if the current one has fewer nulls\n",
    "    if nulls < smallest_nulls:\n",
    "        best_imputation = df_temp.copy()\n",
    "        best_n_neighbors = n_neighbors\n",
    "        smallest_nulls = nulls\n",
    "\n",
    "# Print the best n_neighbors and the corresponding imputation\n",
    "print(f\"Best n_neighbors: {best_n_neighbors}\")\n",
    "print(\"Imputed DataFrame:\")\n",
    "print(best_imputation)\n",
    "# Verify the imputed DataFrame\n",
    "print(best_imputation.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the imputed DataFrame as the DataFrame for the question.\n",
    "df_ML_q1 = best_imputation.copy()\n",
    "df_ML_q1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the features in the Dataset.\n",
    "There are two major factors determining the normalization method of a dataset.  Those factors are: The nature of distribution and outliers.  So, we can plot the features in a graph which would help determine the most appropriate normalization method.\n",
    "\n",
    "Normal Distribution:\n",
    "\n",
    "If your data follows a normal distribution, StandardScaler may be a good choice. StandardScaler assumes that the features are normally distributed and scales them to have a mean of 0 and a standard deviation of 1.\n",
    "If your data deviates significantly from a normal distribution, other scaling methods like MinMaxScaler or RobustScaler may be more appropriate.\n",
    "Outliers:\n",
    "\n",
    "Sensitive to Outliers: \n",
    "\n",
    "MinMaxScaler is sensitive to outliers, meaning that extreme values can disproportionately influence the scaling. If your data contains outliers, StandardScaler may be more robust.\n",
    "Robust to Outliers: RobustScaler is designed to be robust to outliers, making it a suitable choice if your data has extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_ML_q1, diag_kind = \"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on th above graph.  All the features resemble the normal distribution with no outliers.  So, the most appropriate Normalization method is StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_q1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X = df_ML_q1[['Heart_Rate', 'Blood_Pressure', 'Cholesterol', 'Blood_Sugar']]\n",
    "y = df_ML_q1['Risk_Level']\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Now X_train, X_test, y_train, y_test can be used for training and testing your machine learning model\n",
    "print(f'X_Train\\'s Shape: {X_train.shape}')\n",
    "print(f'y_Train\\'s Shape: {y_train.shape}')\n",
    "print(f'X_Test\\'s Shape: {X_test.shape}')\n",
    "print(f'y_Test\\'s Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several machine learning models commonly used for classification problems.  Below are some of the common ones and their suitability.\n",
    "\n",
    "1) Logistic Regression: A simple adn interpretable model suitable for binary and multi-class classification.\n",
    "2) Decision Trees: Easy to understand and interpret, can handle both numerical and categorical data.\n",
    "3) Random Forest: This is an improved version of Decision Trees; an ensemble of decision trees.  Often times more robust and accurate than the individual trees. I handles over-fitting well.\n",
    "4) Support Vector Machines (SVM): Effective in high-dimensional space, good for binary and multi-class classification.\n",
    "5) K-Nearest Neighbors (KNN): This is a non-parametric learning algorithm suitable for both binary and multi-class classification.\n",
    "6) Naive Bayes:  The a fast and simple classification model where it assumes independence between features. \n",
    "\n",
    "Let's test them all and see which one is the most accurate based on the accuracy score: Accuracy = Number of Correct Predictions / Total Number of Predictions \n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Logistic Regression \n",
    "\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l2'], 'solver': ['lbfgs', 'liblinear']}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_logistic_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_logistic = best_logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "# Capture the model name and accuracy score dynamically\n",
    "model_name_accuracy_logistic = {type(best_logistic_model).__name__: accuracy_logistic}\n",
    "print(model_name_accuracy_logistic)\n",
    "\n",
    "# Print a detailed classification report with dynamic model name\n",
    "print(\"Classification Report for\", type(best_logistic_model).__name__, \":\\n\", classification_report(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Decision Tree\n",
    "\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(dt_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dt = best_dt_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Capture the model name and accuracy score dynamically\n",
    "model_name_accuracy_dt = {type(best_dt_model).__name__: accuracy_dt}\n",
    "print(model_name_accuracy_dt)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_dt * 100:.2f}%\")\n",
    "print(\"Classification Report for\", type(best_dt_model).__name__, \":\\n\", classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Random Forest.  This can take up to 5 mins.  \n",
    "\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Capture the model name and accuracy score dynamically\n",
    "model_name_accuracy_rf = {type(best_rf_model).__name__: accuracy_rf}\n",
    "print(model_name_accuracy_rf)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(\"Classification Report for\", type(rf_model).__name__, \":\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Support Vector Machine(SVM)\n",
    "\n",
    "\n",
    "# Create an SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svm = best_svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "model_name_accuracy_svm = {type(best_svm_model).__name__: accuracy_svm}\n",
    "\n",
    "# Print the results with dynamic model name\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy for {type(best_svm_model).__name__}: {accuracy_svm * 100:.2f}%\")\n",
    "print(\"Classification Report for\", type(best_svm_model).__name__, \":\\n\", classification_report(y_test, y_pred_svm, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) K-Nearest Neighbors (KNN)\n",
    "\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(2, 11)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_knn = best_knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "model_name_accuracy_knn = {type(best_knn_model).__name__: accuracy_knn}\n",
    "\n",
    "# Print the results with dynamic model name\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy for {type(best_knn_model).__name__}: {accuracy_knn * 100:.2f}%\")\n",
    "print(\"Classification Report for\", type(best_knn_model).__name__, \":\\n\", classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Naive Bayes\n",
    "\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid_gnb = {\n",
    "    'var_smoothing': [1e-20, 1e-15, 1e-10, 1e-5, 1e-2, 1e-1, 1.0]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, cv=5)\n",
    "grid_search_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_gnb_model = grid_search_gnb.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gnb = best_gnb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "model_name_accuracy_gnb = {type(best_gnb_model).__name__: accuracy_gnb}\n",
    "\n",
    "# Print the results with dynamic model name\n",
    "print(f\"Best Hyperparameters for {type(best_gnb_model).__name__}: {grid_search_gnb.best_params_}\")\n",
    "print(f\"Accuracy for {type(best_gnb_model).__name__}: {accuracy_gnb * 100:.2f}%\")\n",
    "print(\"Classification Report for\", type(best_gnb_model).__name__, \":\\n\", classification_report(y_test, y_pred_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the features with the Risk Level\n",
    "# using  data=df_ML_q1 or best_imputation\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "# List of column to plot\n",
    "columns = ['Heart_Rate', 'Blood_Pressure', 'Cholesterol', 'Blood_Sugar']\n",
    "\n",
    "for i, column in enumerate(columns, 1):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.boxplot(x='Risk_Level', y=column, data=df_ML_q1, order=['Low', 'Medium', 'High'])\n",
    "    plt.title(f'Boxplot of {column} by Risk Level')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the results from the models into a list.\n",
    "\n",
    "\n",
    "# List to store results\n",
    "all_results = []\n",
    "\n",
    "# List of models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machines(SVM)': SVC(random_state=42),\n",
    "    'K-Nearest Neighbours(KNN)': KNeighborsClassifier(),\n",
    "    'Naive Bayes: GaussianNB': GaussianNB(),\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'Logistic Regression':\n",
    "        y_pred = y_pred_logistic\n",
    "        accuracy = accuracy_logistic\n",
    "    elif model_name == 'Decision Tree':\n",
    "        y_pred = y_pred_dt\n",
    "        accuracy = accuracy_dt\n",
    "    elif model_name == 'Random Forest':\n",
    "        y_pred = y_pred_rf\n",
    "        accuracy = accuracy_rf\n",
    "    elif model_name == 'Support Vector Machines(SVM)':\n",
    "        y_pred = y_pred_svm\n",
    "        accuracy = accuracy_svm\n",
    "    elif model_name == 'K-Nearest Neighbours(KNN)':\n",
    "        y_pred = y_pred_knn\n",
    "        accuracy = accuracy_knn\n",
    "    elif model_name == 'Naive Bayes: GaussianNB':\n",
    "        y_pred = y_pred_gnb\n",
    "        accuracy = accuracy_gnb\n",
    "\n",
    "    # Capture the model name, predictions, and accuracy score\n",
    "    model_results = {\n",
    "        'Model Name': model_name,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    # Append the results to the list\n",
    "    all_results.append(model_results)\n",
    "\n",
    "# Print compiled results\n",
    "for result in all_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - best performing model by accuracy\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Identify the best model\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax(), 'Model Name']\n",
    "\n",
    "# Plot the results using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot = sns.barplot(x='Accuracy', y='Model Name', data=results_df, palette='viridis', orient='h')  # orient='h' for horizontal\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Model')\n",
    "plt.xlim(0.0, 0.5)  # Set the x-axis limit between 0 and 0.6 for accuracy\n",
    "plt.yticks(rotation=0)  # Rotate y-axis labels for better visibility\n",
    "\n",
    "# Display the accuracy values inside each bar\n",
    "for index, value in enumerate(results_df['Accuracy']):\n",
    "    plt.text(value, index, f'{value:.2%}', va='center')\n",
    "\n",
    "# Highlight the best model with a dotted line\n",
    "best_model_index = results_df.index[results_df['Model Name'] == best_model].tolist()[0]\n",
    "plot.patches[best_model_index].set_facecolor('red')\n",
    "plt.axvline(results_df.loc[results_df['Model Name'] == best_model, 'Accuracy'].values[0],\n",
    "            color='red', linestyle='--', label=f'Best Model: {best_model}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
