{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55eb3b4b",
   "metadata": {},
   "source": [
    "## ML Final Exam\n",
    "## Question 3: Deep Learning Based Regresion Task (20 Marks):\n",
    "The \"House_Price_Prediction\" dataset is designed to represent properties with features like size,\n",
    "number of bedrooms, number of bathrooms, age, and proximity to the city center. The target variable\n",
    "is the property price. Given the dataset, perform the following tasks to predict the price of properties\n",
    "based on various features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30accb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import warnings\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, InputLayer\n",
    "from keras.regularizers import l2\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51380132",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_q3 = pd.read_excel('House_Price_Prediciton_Data.xlsx')\n",
    "df_ML_q3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ea7fa",
   "metadata": {},
   "source": [
    "### a) Data Preprocessing (4 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data type\n",
    "df_ML_q3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_q3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a46192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "df_ML_q3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5521133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the nature of missing values in 'Age_of_Property', 'Proximity_to_City_Center', and 'Property_Price'\n",
    "missing_Age_of_Property = df_ML_q3['Age_of_Property'].isnull()\n",
    "missing_Proximity_to_City_Center = df_ML_q3['Proximity_to_City_Center'].isnull()\n",
    "missing_Property_Price = df_ML_q3['Property_Price'].isnull()\n",
    "\n",
    "# Check if rows with missing values in 'Age_of_Property' also have missing values in other columns\n",
    "all_missing_values_together = (\n",
    "    missing_Age_of_Property.equals(missing_Proximity_to_City_Center) and\n",
    "    missing_Age_of_Property.equals(missing_Property_Price)\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(f\"All rows with missing 'Age_of_Property' also have missing values in 'Proximity_to_City_Center' and 'Property_Price': {all_missing_values_together}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for irregular values (0) in the column 'Square_Feet'\n",
    "filtered_rows = df_ML_q3[df_ML_q3['Square_Feet'] == 0]\n",
    "\n",
    "# Display the filtered rows\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row number 2902\n",
    "df_ML_q3 = df_ML_q3.drop(df_ML_q3.index[2902])\n",
    "\n",
    "df_ML_q3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b8096",
   "metadata": {},
   "source": [
    "Handling missing values is an important step in preparing your data for a machine learning model. The decision to fill missing values or not depends on the nature of your data and the algorithm you plan to use.\n",
    "\n",
    "Here are a few strategies you can consider:\n",
    "\n",
    "Filling Missing Values:\n",
    "\n",
    "You can fill missing values with the mean, median, or mode of the respective columns. This is a common approach and can be useful when the missing values are missing at random.\n",
    "For the 'Age_of_Property,' you might choose to fill missing values with the mean or median value of the column.\n",
    "For 'Proximity_to_City_Center' and 'Property_Price,' you might consider filling missing values with the mean or median as well.\n",
    "Imputation using Machine Learning:\n",
    "\n",
    "You can use machine learning algorithms to predict missing values based on the other features in your dataset. For example, you could train a regression model to predict 'Age_of_Property' based on 'Square_Feet,' 'Bedrooms,' and 'Bathrooms.' Similarly, you could train models for 'Proximity_to_City_Center' and 'Property_Price.'\n",
    "Handling Missing Values during Training:\n",
    "\n",
    "Some machine learning algorithms, like decision trees or deep learning models, can handle missing values during training. In this case, you don't need to fill missing values explicitly, as the algorithm will learn how to handle them.\n",
    "Removing Rows:\n",
    "\n",
    "If the missing values are a small percentage of your data and removing those rows doesn't significantly impact your dataset's representativeness, you might consider removing those rows.\n",
    "The best approach depends on the characteristics of your data, the distribution of missing values, and the requirements of your model. Experiment with different strategies and evaluate their impact on your model's performance.\n",
    "\n",
    "For deep learning models, make sure to normalize or standardize your data and handle missing values appropriately. It's also essential to split your dataset into training, validation, and test sets to evaluate your model's performance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with negative 'Property_Price'\n",
    "df_ML_q3 = df_ML_q3[df_ML_q3['Property_Price'] >= 0]\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df_ML_q3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_ML_q3.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2452e3",
   "metadata": {},
   "source": [
    "### Imputing the missing values with IterativeImputer(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for imputation\n",
    "cols_to_impute = ['Square_Feet', 'Bedrooms', 'Bathrooms', 'Age_of_Property', 'Proximity_to_City_Center', 'Property_Price']\n",
    "imputation_df = df_ML_q3[cols_to_impute]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(imputation_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values using an iterative imputer with linear regression\n",
    "imputer = IterativeImputer(estimator=LinearRegression(), random_state=42, max_iter=100)\n",
    "imputed_train_data = imputer.fit_transform(train_data)\n",
    "imputed_test_data = imputer.transform(test_data)\n",
    "\n",
    "# Convert imputed data to DataFrames\n",
    "imputed_train_df, imputed_test_df = pd.DataFrame(imputed_train_data, columns=cols_to_impute), pd.DataFrame(imputed_test_data, columns=cols_to_impute)\n",
    "\n",
    "# Update original DataFrame with imputed values\n",
    "df_ML_q3.loc[train_data.index, cols_to_impute] = imputed_train_df\n",
    "df_ML_q3.loc[test_data.index, cols_to_impute] = imputed_test_df\n",
    "\n",
    "# Create a new DataFrame with imputed values\n",
    "df_ML_q3_LiRegression_imputed = df_ML_q3.copy()\n",
    "df_ML_q3_LiRegression_imputed[cols_to_impute] = imputer.transform(df_ML_q3_LiRegression_imputed[cols_to_impute])\n",
    "\n",
    "# Print shape, check for dropped rows, and display head & description\n",
    "print(df_ML_q3_LiRegression_imputed.shape)\n",
    "df_ML_q3_LiRegression_imputed.head(5)\n",
    "df_ML_q3_LiRegression_imputed.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution after imputation\n",
    "\n",
    "\n",
    "# Determine the threshold for 'Age of Property'\n",
    "age_threshold = df_ML_q3['Age_of_Property'].quantile(0.95) # or 0.99 if you want to be more conservative\n",
    "# Filter out the outliers\n",
    "df_ML_q3_filtered = df_ML_q3[df_ML_q3['Age_of_Property'] <= age_threshold]\n",
    "\n",
    "# Determine the threshold for 'Proximity to City Center'\n",
    "proximity_threshold = df_ML_q3['Proximity_to_City_Center'].quantile(0.95) # or 0.99\n",
    "\n",
    "# Filter out the outliers\n",
    "df_ML_q3_filtered = df_ML_q3_filtered[df_ML_q3_filtered['Proximity_to_City_Center'] <= proximity_threshold]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for 'Age_of_Property' after filtering out outliers\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "sns.histplot(df_ML_q3_filtered['Age_of_Property'], kde=True, color='blue', bins=30)\n",
    "plt.title('Age of property')\n",
    "plt.xlabel('Age of property')\n",
    "plt.ylabel('Frequency')\n",
    "# Format the x-axis tick labels to show the actual values\n",
    "ax1.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "# Rotate the x-axis labels\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for 'Proximity_to_City_Center' after filtering out outliers\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "sns.histplot(df_ML_q3_filtered['Proximity_to_City_Center'], kde=True, color='blue', bins=30)\n",
    "plt.title('Proximity to city center')\n",
    "plt.xlabel('Proximity to city center')\n",
    "plt.ylabel('Frequency')\n",
    "# Format the x-axis tick labels to show the actual values\n",
    "ax2.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "# Rotate the x-axis labels\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot for 'Property_Price' before imputation\n",
    "ax = plt.subplot(2,2,3)\n",
    "sns.histplot(df_ML_q3['Property_Price'], kde=True, color='blue', bins=30)\n",
    "plt.title('Property price')\n",
    "plt.xlabel('Property price')\n",
    "plt.ylabel('Frequency')\n",
    "# Format the x-axis tick labels to show the actual values\n",
    "ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "# Rotate the x-axis labels\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure that Missing values are imputed.\n",
    "df_ML_q3_LiRegression_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - Price distribution by the number of bedroom and bathroom\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = sns.boxplot(x='Bedrooms', y='Property_Price', data=df_ML_q3)\n",
    "plt.title('Property Price Distribution by Number of Bedrooms')\n",
    "# Format the y-axis label to show actual prices\n",
    "ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = sns.boxplot(x='Bathrooms', y='Property_Price', data=df_ML_q3)\n",
    "plt.title('Property Price Distribution by Number of Bathrooms')\n",
    "# Format the axis label\n",
    "ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81918b96",
   "metadata": {},
   "source": [
    "### Defining X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the independent X and dependent variables y\n",
    "\n",
    "# storing all the independent variables as X\n",
    "X = df_ML_q3_LiRegression_imputed.drop('Property_Price', axis=1)\n",
    "\n",
    "# storing the dependent variable as y\n",
    "y = df_ML_q3_LiRegression_imputed['Property_Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234d63f",
   "metadata": {},
   "source": [
    "### Preparing X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ff625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform X\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f22278",
   "metadata": {},
   "source": [
    "### Create Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8598955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# shape of training and validation set\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0992985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_test))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753780c",
   "metadata": {},
   "source": [
    "## b) Model Building (10 Marks)\n",
    "## Our best model first and the one discussed further in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback to print validation loss\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        print(f'Epoch {epoch + 1}: val_loss = {val_loss:.5f}')\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Hidden Layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model (defining loss function, optimizer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Define custom callback to print validation loss\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "# Train the model with early stopping and custom callback\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, custom_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Report metrics on the test set\n",
    "print(f\"\\nTest Mean Squared Error: {test_loss}\")\n",
    "print(f\"Test Mean Absolute Error: {test_mae}\")\n",
    "print(f\"R^2 Score on Test Set: {r2}\")\n",
    "\n",
    "# Visualization of true vs predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='blue', label='Predicted')\n",
    "plt.scatter(y_test, y_test, alpha=0.5, color='red', label='True Value')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0, plt.xlim()[1]])\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "plt.legend()\n",
    "plt.plot([-100, 5000], [-100, 5000], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance on the test set is quite impressive:\n",
    "\n",
    "Test Mean Squared Error (MSE): The mean squared error is a measure of the average squared difference between the predicted values and the true values. In this case, the test MSE is relatively low at 20,786,412 indicating that, on average, the model's predictions are close to the true values.\n",
    "\n",
    "R^2 Score on Test Set: The R-squared (R^2) score is a measure of how well the predicted values match the actual values. A score of 0.997 is very close to 1, which suggests that the model explains a high proportion of the variance in the test data. In other words, the model's predictions are highly accurate and align well with the true values.\n",
    "\n",
    "In summary, the low MSE and high R^2 score indicate that the model has performed very well on the test set and has learned patterns in the data that generalize effectively to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Deep Learning Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f24232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a custom callback to print validation loss\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get('val_loss')\n",
    "        print(f'Epoch {epoch + 1}: val_loss = {val_loss:.5f}')\n",
    "\n",
    "# Using X_train dimentions\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Add dropout layer\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Add dropout layer\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Add dropout layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define model checkpoint callback to save the best model\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Define custom callback to print validation loss\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "# Train the model with early stopping and custom callback\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint, custom_callback])\n",
    "\n",
    "\n",
    "print('\\nBest config Summary')\n",
    "model.summary()\n",
    "\n",
    "# Calculate R^2 score\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\n R^2 Score: {r2}\")\n",
    "\n",
    "# Evaluate the model loss and metrics on the test set\n",
    "test_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Extract individual metrics\n",
    "test_loss = test_results[0]\n",
    "test_mse = test_results[1]\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "# Create variables for visualization\n",
    "actual_values = y_test\n",
    "\n",
    "# Flatten predictions to match the shape of actual_values\n",
    "predicted_values = y_pred.flatten()  \n",
    "\n",
    "# Plotting actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='blue', label='Predicted')\n",
    "plt.scatter(y_test, y_test, alpha=0.5, color='red', label='True Value')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0, plt.xlim()[1]])\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "plt.legend()\n",
    "plt.plot([-100, 5000], [-100, 5000], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec025301",
   "metadata": {},
   "source": [
    "This model has three dense layers and dropout layers in between. It has a total of 17,025 parameters.\n",
    "\n",
    "This runs until it has 2 consecutive non-improving epochs. The scoring metrics might change when run.\n",
    "\n",
    "R^2 Score: The R^2 score on the test set is approximately 0.93. This is an excellent R^2 score, suggesting that the model explains a very high proportion of the variance in the target variable.\n",
    "\n",
    "Test Metrics: The test set evaluation shows a test loss (MSE) of 559,731,008 and a Mean Absolute Error (MAE) of 487,240,384 The MSE is relatively high, but the scale of the target variable needs to be considered to interpret the significance of this value. The MAE is a measure of the average absolute errors between the predicted and actual values.\n",
    "\n",
    "Overall, based on the provided metrics, the model appears to have learned the patterns in the data very well and is performing at a high level. The R^2 score of 0.9865 indicates a strong correlation between predicted and actual values. However, it's crucial to consider the specific requirements and context of your problem to determine whether this level of performance is acceptable for your application. If these results meet your criteria, then the model can be considered successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Deep Learning Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad5750",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Hidden Layers with Dropout for Regularization\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model with a custom learning rate and mean absolute error loss\n",
    "custom_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_absolute_error', metrics=['mse'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define a callback to save the best model during training\n",
    "checkpoint_filepath = 'best_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Train the model with the defined callback\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[model_checkpoint])\n",
    "\n",
    "# Save the best model in the native Keras format\n",
    "model.save('best_model.keras')\n",
    "\n",
    "# Load the best model\n",
    "best_model = keras.models.load_model('best_model.keras')\n",
    "\n",
    "# Print details of the best model\n",
    "print(\"Details of the Best Model:\")\n",
    "best_model.summary()\n",
    "\n",
    "# Prediction\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 score\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"R^2 Score: {score}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_results = best_model.evaluate(X_test, y_test)\n",
    "test_loss = test_results[0]\n",
    "test_mse = test_results[1]\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "# Visualization of true vs predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='blue', label='Predicted')\n",
    "plt.scatter(y_test, y_test, alpha=0.5, color='red', label='True Value')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0, plt.xlim()[1]])\n",
    "plt.ylim([0, plt.ylim()[1]])\n",
    "plt.legend()\n",
    "plt.plot([-100, 5000], [-100, 5000], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot - True vs Predicted Values Distribution KDE\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# Plotting the KDE for true values\n",
    "sns.kdeplot(y_test, color='blue', label='True Values', fill=True, ax=ax)\n",
    "\n",
    "# Plotting the KDE for predicted values\n",
    "sns.kdeplot(y_pred.flatten(), color='red', label='Predicted Values', fill=True, ax=ax)\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('True vs Predicted Values Distribution - KDE')\n",
    "\n",
    "# Set x-axis limits to avoid negative values\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "# Format the x-axis tick labels to show the actual values\n",
    "ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
